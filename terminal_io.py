""" A frontend of LLM-OS, it's just a prototype for now."""

import argparse
import json
import logging
import os
from datetime import datetime

import pyperclip
from dotenv import load_dotenv
from prompt_toolkit import PromptSession, prompt
from prompt_toolkit.completion import Completer, Completion
from prompt_toolkit.key_binding import KeyBindings
from prompt_toolkit.keys import Keys
from prompt_toolkit.shortcuts import input_dialog, clear
from prompt_toolkit.styles import Style
from prompt_toolkit.validation import ValidationError, Validator
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel

from chatgpt_tool_hub.apps import AppFactory, App
from chatgpt_tool_hub.common.calculate_token import count_message_tokens
from chatgpt_tool_hub.common.constants import LOGGING_FMT, LOGGING_DATEFMT
from chatgpt_tool_hub.tools.all_tool_list import main_tool_register
from chatgpt_tool_hub.tools.news import news_tool_register

logging.basicConfig(filename=f'{os.getcwd()}/llmos.log', format=LOGGING_FMT,
                    datefmt=LOGGING_DATEFMT, level=logging.INFO)

LOG = logging.getLogger("llmos")

console = Console()

style = Style.from_dict({
    "prompt": "ansigreen",  # å°†æç¤ºç¬¦è®¾ç½®ä¸ºç»¿è‰²
})

input_dialog_style = Style.from_dict({
    'dialog': 'bg:#000000',
    'dialog.body': 'bg:#ffffff',
})

who = ""

init_chat_history = []

tools_with_api_key = {
    "bing-search": ["bing_subscription_key"],
    "google-search": ["google_api_key", "google_cse_id"],
    "searxng-search": ["searx_search_host"],
    "morning-news": ["morning_news_api_key"],
    "news-api": ["news_api_key"],
    "wolfram-alpha": ["wolfram_alpha_appid"],
}

# todo consider higher tree
# Dynamic addition and deletion of subtools are currently not supported.
subtool_parent = {
    "morning-news": "news",
    "news-api": "news"
}


def read_config_json() -> dict:
    curdir = os.path.dirname(__file__)
    config_path = os.path.join(curdir, "config.json")
    tool_config = {"tools": [], "kwargs": {"nolog": True}}
    if not os.path.exists(config_path):
        return tool_config
    with open(config_path, "r") as f:
        tool_config = json.load(f)
    # todo test it
    tool_config.get("kwargs", {})["nolog"] = True

    return tool_config


def save_config_json():
    global config
    curdir = os.path.dirname(__file__)
    config_path = os.path.join(curdir, "config.json")
    with open(config_path, "w") as f:
        json.dump(config, f)


config = read_config_json()


class ChatMode:
    debug_mode = False
    raw_mode = False
    multi_line_mode = False

    @classmethod
    def toggle_debug_mode(cls):
        cls.debug_mode = not cls.debug_mode
        console.print(
            f"[dim]Debug æ¨¡å¼ {'å·²å¼€å¯' if cls.debug_mode else 'å·²å…³é—­'}")

    @classmethod
    def toggle_raw_mode(cls):
        cls.raw_mode = not cls.raw_mode
        console.print(
            f"[dim]Raw æ¨¡å¼ {'å·²å¼€å¯' if cls.raw_mode else 'å·²å…³é—­'}, "
            "ä½¿ç”¨ `/last` æ¥æ˜¾ç¤º LLM-OS ä¸Šæ¬¡å›å¤ .")

    @classmethod
    def toggle_multi_line_mode(cls):
        cls.multi_line_mode = not cls.multi_line_mode
        if cls.multi_line_mode:
            console.print(
                "[dim]Multi-line æ¨¡å¼ å·²å¼€å¯, ä½¿ç”¨ [[bright_magenta]Esc[/]] + [[bright_magenta]ENTER[/]] æäº¤è·¨è¡Œæ–‡æœ¬."
            )
        else:
            console.print("[dim]Multi-line æ¨¡å¼ å·²å…³é—­.")


class LLMOS:
    def __init__(self, timeout: int):
        self.messages = init_chat_history

        self.app = self.create_app()

        self.model = 'gpt-35-turbo'
        self.timeout = timeout

        # todo éœ€è¦llm-os modelæ”¯æŒ
        self.tokens_limit = 4096
        self.total_tokens_spent = 0
        self.current_tokens = count_message_tokens(self.messages)

    def create_app(self):
        return AppFactory().create_app(tools_list=config["tools"], console=console, **config["kwargs"])

    @property
    def get_app(self) -> App:
        return self.app

    def handle(self, message: str):
        try:
            self.messages.append({"role": "user", "content": message})

            response = self.send_request(message)

            if response is None:
                self.messages.pop()
                return

            LOG.info(f"LLM-OS response: {response}")
            _message = {"role": "assistant", "content": f"{response}"}
            print_message(_message)
            self.messages.append(_message)
            # todo
            self.current_tokens = count_message_tokens(self.messages)
            self.total_tokens_spent += self.current_tokens

        except Exception as e:
            console.print(f"[red]ç³»ç»Ÿé”™è¯¯: {str(e)}. è¯·æ£€æŸ¥æ—¥å¿—è·å–æ›´å¤šä¿¡æ¯")
            LOG.exception(e)
            # todo
            self.save_chat_history(f'chat_history_backup_{datetime.now().strftime("%Y-%m-%d_%H,%M,%S")}.json')
            raise EOFError

        return response

    def send_request(self, message: str):
        try:
            with console.status("[bold cyan]LLM-OS åˆ†æä¸­...\n", spinner="earth"):
                response = self.app.ask(message, chat_history=self.messages)

            return response
        except KeyboardInterrupt:
            console.print("[bold cyan]ä¸»åŠ¨ä¸­æ–­. åˆ†æå·²åœæ­¢.")
            raise
        except Exception as e:
            console.print(f"[red]é”™è¯¯: {str(e)}. ")
            LOG.exception(e)
            return None

    def save_chat_history(self, filename):
        # é»˜è®¤å­˜æ”¾è·¯å¾„åœ¨æœ¬æ–‡ä»¶ä¸‹çš„logç›®å½•
        file_dir_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "log")
        if not os.path.exists(file_dir_path):
            os.mkdir(file_dir_path)
        try:
            with open(f"{os.path.join(file_dir_path, filename)}", 'w', encoding='utf-8') as f:
                json.dump(self.messages, f, ensure_ascii=False, indent=4)
            console.print(f"[dim]èŠå¤©è®°å½•ä¿å­˜æˆåŠŸ: [bright_magenta]{filename}", highlight=False)
        except Exception as e:
            console.print(f"[red]èŠå¤©è®°å½•ä¿å­˜å¤±è´¥: {str(e)}. ")
            LOG.exception(e)

    def get_credit_usage(self):
        # todo
        # tool-hubæš‚æ—¶ä¸æ”¯æŒç»Ÿè®¡tokens cost
        return {
            'total_granted': "tool-hubæš‚æ—¶ä¸æ”¯æŒç»Ÿè®¡tokens cost",
            'total_used': '',
            'total_available': ''
        }

    def modify_system_prompt(self, new_content):
        # todo
        # tool-hubæš‚æ—¶ä¸æ”¯æŒä¿®æ”¹ç³»ç»Ÿprompt
        return new_content

    def set_model(self, new_model: str):
        old_model = self.model
        if not new_model:
            console.print(f"[dim]æˆ‘æ²¡æœ‰æ”¶åˆ°æ–°æ¨¡å‹åï¼Œæ¨¡å‹æœªå˜æ›´: [bold cyan]{old_model}[/].")
            return
        # todo test gpt-4
        if self.model.startswith("gpt-4-32k"):
            tokens_limit = 32768
        elif self.model.startswith("gpt-4"):
            tokens_limit = 8192
        elif self.model.startswith("gpt-35-turbo"):
            tokens_limit = 4096
        else:
            console.print(f"[red]æ²¡æœ‰è¯¥æ¨¡å‹ {new_model} tokensä¿¡æ¯ï¼Œæ¨¡å‹æœªå˜æ›´: [bold cyan]{old_model}[/].")
            return

        config["kwargs"]["model_name"] = new_model
        self.model = new_model
        self.tokens_limit = tokens_limit
        console.print(f"[dim]æ¨¡å‹å°†å‘ç”Ÿå˜æ›´ [bold cyan]{old_model}[/] -> [bold red]{new_model}[/].")
        self.app = self.create_app()

    def set_timeout(self, timeout):
        old_timeout = self.timeout
        try:
            self.timeout = float(timeout)
        except ValueError:
            console.print("[red]æˆ‘æ²¡æœ‰æ”¶åˆ°æ•°å­—")
            return
        config["kwargs"]["request_timeout"] = self.timeout
        console.print(f"[dim] LLM-OSè¶…æ—¶æ—¶é—´å°†å‘ç”Ÿå˜æ›´ [bold cyan]{old_timeout}[/] -> [bold red]{self.timeout}[/].")
        self.app = self.create_app()


class CustomCompleter(Completer):
    commands = [
        '/debug', '/raw', '/multi', '/tool', '/add', '/del', '/depth', '/reset', '/model',
        '/last', '/save', '/clear', '/timeout', '/undo', '/exit', '/copy', '/help'
    ]

    available_models = [
        "gpt-35-turbo",
        "gpt-35-turbo-0301",
        "gpt-35-turbo-0613",
        "gpt-35-turbo-16k",
        "gpt-35-turbo-16k-0613",
        "gpt-4",
        "gpt-4-0314",
        "gpt-4-32k",
        "gpt-4-32k-0314",
    ]

    def get_completions(self, document, complete_event):
        text = document.text_before_cursor
        if text.startswith('/'):
            # Check if it's a /model command
            if text.startswith('/model '):
                model_prefix = text[7:]
                for model in self.available_models:
                    if model.startswith(model_prefix):
                        yield Completion(model, start_position=-len(model_prefix))
            if text.startswith('/add '):
                tool_prefix = text[5:]
                available_tools = main_tool_register.get_registered_tool_names() \
                                  + news_tool_register.get_registered_tool_names()
                for tool in available_tools:
                    if tool.startswith(tool_prefix):
                        yield Completion(tool, start_position=-len(tool_prefix))
            else:
                for command in self.commands:
                    if command.startswith(text):
                        yield Completion(command, start_position=-len(text))


class NumberValidator(Validator):
    def validate(self, document):
        text = document.text
        if not text.isdigit():
            raise ValidationError(message="Please input an Integer!", cursor_position=len(text))


def print_message(message):
    """æ‰“å°å•æ¡æ¥è‡ª LLM-OS æˆ–ç”¨æˆ·çš„æ¶ˆæ¯"""
    role = message["role"]
    content = message["content"]
    if role == "user":
        print(f"> {who}: {content}")
    elif role == "assistant":
        # todo æœ‰æ—¶ä¼šåæ•°æ®
        console.print("LLM-OS: ", end='', style="bold cyan")
        if ChatMode.raw_mode:
            print(content)
        else:
            console.print(Markdown(content), new_line_start=True)


def handle_command(command: str, llm_os: LLMOS):
    """å¤„ç†æ–œæ (/)å‘½ä»¤"""
    global config

    if command == '/raw':
        ChatMode.toggle_raw_mode()
    elif command == '/multi':
        ChatMode.toggle_multi_line_mode()
    elif command == '/debug':
        ChatMode.toggle_debug_mode()
        config["kwargs"]['nolog'] = not ChatMode.debug_mode
        config["kwargs"]['debug'] = ChatMode.debug_mode
        llm_os.app = llm_os.create_app()
    elif command == '/tool':
        tools_list = llm_os.get_app.get_tool_list()
        # todo beautify below Panel
        console.print(Panel(repr(tools_list), title='å·¥å…·åˆ—è¡¨', style='dim'))

    elif command.startswith('/add'):
        args = command.split()
        if len(args) > 1:
            add_tool = args[1]
        else:
            add_tool = prompt("è¯·è¾“å…¥æƒ³è¦æ·»åŠ çš„å·¥å…·å: ", default="", style=style)

        if not add_tool:
            console.print("toolæœªæ”¹å˜.")
            return

        tools_kwargs = {}
        if tools_with_api_key.get(add_tool):
            console.print(f"æ·»åŠ  [bold cyan]{add_tool}[/] å·¥å…·å¿…é¡»é¢å¤–ç”³è¯·æœåŠ¡key")
            for tool_args in tools_with_api_key.get(add_tool):
                add_tool_args = ""
                while not add_tool_args:
                    # ControlC or ControlD to break this loop
                    add_tool_args = prompt(f"{tool_args}: ", default="", style=style)
                tools_kwargs[tool_args] = add_tool_args

        # todo ç›®å‰tool-hubä¸æ”¯æŒsubtoolç²’åº¦å¢åˆ tool
        if add_tool not in main_tool_register.get_registered_tool_names():
            if add_tool not in subtool_parent.keys():
                console.print(f"å‘ç°æœªçŸ¥å·¥å…·: {add_tool}")
                return
            elif subtool_parent[add_tool] not in llm_os.get_app.get_tool_list():
                add_tool = subtool_parent[add_tool]

        app = llm_os.get_app
        app.add_tool(add_tool, **tools_kwargs)

        tools_list = app.get_tool_list()
        # todo beautify below Panel
        console.print(Panel(repr(tools_list), title='å·¥å…·åˆ—è¡¨', style='dim'))

    elif command.startswith('/del'):
        args = command.split()
        if len(args) > 1:
            del_tool = args[1]
        else:
            del_tool = prompt("è¯·è¾“å…¥æƒ³è¦åˆ é™¤çš„å·¥å…·å: ", default="", style=style)

        if not del_tool:
            console.print("toolæœªæ”¹å˜.")
            return

        app = llm_os.get_app
        app.del_tool(del_tool)

        tools_list = app.get_tool_list()
        # todo beautify below Panel
        console.print(Panel(repr(tools_list), title='å·¥å…·åˆ—è¡¨', style='dim'))

    elif command.startswith('/depth'):
        args = command.split()
        if len(args) > 1:
            new_think_depth = args[1]
        else:
            new_think_depth = prompt("è¯·è¾“å…¥LLM-OSè®¾å®šçš„æ€è€ƒæ·±åº¦: ", default="2", style=style)

        if not new_think_depth:
            console.print("depthæœªæ”¹å˜.")
            return

        try:
            new_think_depth = int(new_think_depth)
        except Exception as e:
            LOG.error(f"parsing new_think_depth error: {repr(e)}")
            console.print("æ€è€ƒæ·±åº¦ç±»å‹å¿…é¡»ä¸ºæ•´æ•°ï¼Œdepthæœªæ”¹å˜.")

        app = llm_os.get_app
        app.think_depth = new_think_depth
        app.load_tools_into_bot()

    elif command == '/reset':
        # todo bug è¿™é‡Œæ²¡æœ‰æ¸…ç©ºå†å²è®°å½•
        config = read_config_json()
        llm_os.app = llm_os.create_app()
        llm_os.messages = init_chat_history

    elif command.startswith('/model'):
        args = command.split()
        if len(args) > 1:
            new_model = args[1]
        else:
            new_model = prompt("è¯·è¾“å…¥è¦æ›´æ”¹çš„LLMåç§°: ", default=llm_os.model, style=style)

        if new_model != llm_os.model:
            llm_os.set_model(new_model)
        else:
            console.print("[dim]modelæœªæ”¹å˜.")

    elif command == '/last':
        if len(llm_os.messages) > 1:
            reply = llm_os.messages[-1]
            print_message(reply)
        else:
            console.print("[dim]æ²¡æœ‰è¦åšçš„äº‹æƒ….")

    elif command.startswith('/save'):
        args = command.split()
        if len(args) > 1:
            filename = args[1]
        else:
            date_filename = f'chat_history_{datetime.now().strftime("%Y-%m-%d_%H,%M,%S")}.json'
            filename = prompt("Save to: ", default=date_filename, style=style)
        llm_os.save_chat_history(filename)

    elif command == '/clear':
        clear()

    elif command.startswith('/timeout'):
        args = command.split()
        if len(args) > 1:
            new_timeout = args[1]
        else:
            new_timeout = prompt("è¯·è¾“å…¥æ›´æ”¹åçš„è¶…æ—¶æ—¶é—´ [bold red]æ•´æ•°[/]: ", default=str(llm_os.timeout), style=style)
        if new_timeout != str(llm_os.timeout):
            llm_os.set_timeout(new_timeout)
        else:
            console.print("[dim]timeoutæœªæ”¹å˜. ")

    elif command == '/undo':
        if len(llm_os.messages) >= 2:
            question = llm_os.messages.pop()
            if question['role'] == "assistant":
                question = llm_os.messages.pop()
            # print undo question
            truncated_question = question['content'].split('\n')[0]
            if len(question['content']) > len(truncated_question):
                truncated_question += "..."
            console.print(f"[dim]ä¸Šä¸ªé—®é¢˜: [bold dim]'{truncated_question}'[/] å’Œå¯¹åº”å›å¤å·²æ¸…é™¤")
        else:
            console.print("[dim]æ²¡æœ‰è¦åšçš„äº‹æƒ….")

    elif command.startswith('/copy'):
        if len(llm_os.messages) > 1:
            reply = llm_os.messages[-1]
            pyperclip.copy(reply["content"])
            console.print("[dim]LLM-OSä¸Šæ¬¡å›å¤å·²å¤åˆ¶åˆ°ç²˜è´´æ¿")
        else:
            console.print("[dim]æ²¡æœ‰è¦åšçš„äº‹æƒ….")

    elif command == '/exit':
        raise EOFError

    else:
        # todo ä¸º /help ä¸“é—¨åšä¸€ä¸ªé¡µé¢
        console.print("""[bold]Available commands:[/]
    /debug                   - åˆ‡æ¢debugæ¨¡å¼å¼€å…³
    /raw                     - åˆ‡æ¢rawæ¨¡å¼å¼€å…³ (ç¦ç”¨å¯Œæ–‡æœ¬)
    /multi                   - åˆ‡æ¢multi-lineæ¨¡å¼å¼€å…³ (å…è®¸å¤šè¡Œè¾“å…¥)
    /tool                    - æŸ¥çœ‹å½“å‰åŠ è½½å·¥å…·åˆ—è¡¨
    /add     \[tool_name]     - å¢åŠ å·¥å…·
    /del     \[tool_name]     - åˆ é™¤å·¥å…·
    /depth                   - è®¾ç½®LLM-OSæ€è€ƒæ·±åº¦ (è®¾ç½®è¿‡å¤§å¯èƒ½æ— æ³•åœæ­¢)
    /reset                   - LLM-OSé‡ç½® (é‡æ–°åŠ è½½é…ç½®å¹¶é‡ç½®èŠå¤©è®°å½•)
    /timeout \[new_timeout]   - ä¿®æ”¹è®¿é—®llmçš„è¯·æ±‚è¶…æ—¶æ—¶é—´
    /model   \[model_name]    - åˆ‡æ¢æ¨¡å‹ (ç›®å‰ä»…æ”¯æŒgpt-35)
    /last                    - æ˜¾ç¤ºä¸Šä¸€æ¬¡LLM-OSçš„å›å¤å†…å®¹
    /copy                    - å¤åˆ¶ä¸Šä¸€æ¬¡LLM-OSçš„å›å¤å†…å®¹åˆ°ç²˜è´´æ¿
    /undo                    - æ¸…é™¤ä¸Šä¸€æ¬¡ä¸llmçš„å¯¹è¯è®°å½• (åŒ…å«é—®é¢˜å’Œå›å¤)
    /save    \[filename]      - ä¿å­˜èŠå¤©è®°å½•
    /clear                   - æ¸…å±
    /exit                    - ç¦»å¼€
    /help                    - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯""")


def create_key_bindings():
    """è‡ªå®šä¹‰å›è½¦äº‹ä»¶ç»‘å®šï¼Œå®ç°æ–œæ å‘½ä»¤çš„æäº¤å¿½ç•¥å¤šè¡Œæ¨¡å¼"""
    key_bindings = KeyBindings()

    @key_bindings.add(Keys.Enter, eager=True)
    def _(event):
        buffer = event.current_buffer
        text = str(buffer.text).strip()
        if text.startswith('/') or not ChatMode.multi_line_mode:
            buffer.validate_and_handle()
        else:
            buffer.insert_text('\n')

    return key_bindings


def main(args):
    # ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    if args.key:
        api_key = str(args.key)
    elif config.get("kwargs", {}).get("llm_api_key", ""):
        api_key = config["kwargs"]["llm_api_key"]
    else:
        api_key = os.environ.get("LLM_API_KEY")
    if not api_key:
        api_key = prompt("æˆ‘æ²¡æœ‰æ‰¾åˆ°OpenAI API Key, è¯·è¾“å…¥: ", style=style)
    os.environ["LLM_API_KEY"] = api_key

    if args.timeout:
        request_timeout = args.timeout
    elif config.get("kwargs", {}).get("request_timeout", ""):
        request_timeout = config["kwargs"]["request_timeout"]
    else:
        request_timeout = os.environ.get("REQUEST_TIMEOUT")

    request_timeout = int(request_timeout) if request_timeout else 90
    os.environ["REQUEST_TIMEOUT"] = str(request_timeout)

    if args.debug or str(os.environ.get("DEBUG")).lower() in {
        'true',
        'enable',
        'yes',
    }:
        ChatMode.toggle_debug_mode()

    if args.multi:
        ChatMode.toggle_multi_line_mode()

    if args.raw:
        ChatMode.toggle_raw_mode()

    global who
    if not ChatMode.debug_mode:
        who = input_dialog(
            title='ä¸ªæ€§åŒ–è®¾ç½®',
            text='è®©æˆ‘çŸ¥é“ä½ çš„åå­—: ',
            ok_text='ç¡®è®¤',
            cancel_text='è·³è¿‡',
            style=input_dialog_style).run()
    if not who:
        who = 'user'

    config["kwargs"]["human_prefix"] = who

    llm_os = LLMOS(request_timeout)

    if args.model:
        llm_os.set_model(args.model)

    clear()

    console.print(
        f"[dim]{who} ä½ å¥½ :wave:  æ¬¢è¿è¿›å…¥ LLM-OS! \n"
        "è¾“å…¥ `[bright_magenta]/help[/]` å¯ä»¥è·å¾—å¸®åŠ©ä¿¡æ¯ \n"
        "ç›®å‰LLM-OSå¼€å‘è€…åªæœ‰æˆ‘ èƒ½é¢„è§æœ‰å¤§é‡:wrench:ä¸èƒ½å…¼é¡¾ è¯·è§è°… :persevere: \n"
        "æ¬¢è¿æissueå’Œpr å¸Œæœ›è¿™ä¸ªé¡¹ç›®å˜å¾—æ›´å¥½ :chart_with_upwards_trend:"
    )

    session = PromptSession()

    # è‡ªå®šä¹‰å‘½ä»¤è¡¥å…¨ï¼Œä¿è¯è¾“å…¥â€˜/â€™åç»§ç»­æ˜¾ç¤ºè¡¥å…¨
    commands = CustomCompleter()

    # ç»‘å®šå›è½¦äº‹ä»¶ï¼Œè¾¾åˆ°è‡ªå®šä¹‰å¤šè¡Œæ¨¡å¼çš„æ•ˆæœ
    key_bindings = create_key_bindings()

    while True:
        try:
            message = session.prompt(f'> {who}: ', completer=commands,
                                     complete_while_typing=True, key_bindings=key_bindings)

            if message.startswith('/'):
                command = str(message).strip().lower()
                handle_command(command, llm_os)
            else:
                if not message:
                    continue

                if message.lower() in ['bye', 'goodbye', 'end', 'exit', 'quit']:
                    break

                LOG.info(f"> {who}: {message}")
                llm_os.handle(message)

        except KeyboardInterrupt:
            # raises KeyboardInterrupt when ControlC has been pressed
            continue
        except EOFError:
            # EOFError when ControlD has been pressed
            break

    save_config_json()
    console.print("[bright_magenta]æ‹œæ‹œ~ ğŸ‘‹ğŸ»")
    # todo
    # LOG.info(f"è¿™æ¬¡äº’åŠ¨ç”¨äº† {llm_os.total_tokens_spent} tokens")
    # console.print(
    #     f"[bright_magenta]è¿™æ¬¡äº’åŠ¨ç”¨äº†:  [bold]{llm_os.total_tokens_spent} tokens")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Welcome to chat with LLM-OS.')
    parser.add_argument('-k', '--key', type=str, help='OpenAI API key to load')
    parser.add_argument('-t', '--timeout', type=int, help='set llm request timeout')
    parser.add_argument('--model', type=str, help='choose the AI model to use')
    parser.add_argument('-d', '--debug', action='store_true',
                        help='Enable debug mode')
    parser.add_argument('-m', '--multi', action='store_true',
                        help='Enable multi-line mode')
    parser.add_argument('-r', '--raw', action='store_true',
                        help='Enable raw mode')
    args = parser.parse_args()

    main(args)
